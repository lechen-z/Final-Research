#include "hierarchical_parallel_framework.h"
#include "data_loader.h"
#include <iostream>
#include <vector>
#include <chrono>
#include <iomanip>
#include <cmath>

#ifdef __linux__
#include <sys/sysinfo.h>
#endif

#ifdef USE_CUDA
#include <cuda_runtime.h>
#endif

#ifdef USE_MPI
#include <mpi.h>
#endif

using namespace HierarchicalParallel;
using namespace DataLoader;

// æ£€æµ‹ç¡¬ä»¶é…ç½®
HardwareConfig detectHardwareConfig() {
    HardwareConfig config;
    
    // æ£€æµ‹CPUæ ¸å¿ƒæ•°
    config.num_cores = std::thread::hardware_concurrency();
    if (config.num_cores == 0) config.num_cores = 4; // é»˜è®¤å€¼
    
    // æ£€æµ‹AVXæ”¯æŒ
#ifdef __AVX512F__
    config.has_avx512 = true;
#elif defined(__AVX2__)
    config.has_avx512 = false;
#endif
    
    // æ£€æµ‹CUDAæ”¯æŒ
#ifdef USE_CUDA
    int device_count = 0;
    if (cudaGetDeviceCount(&device_count) == cudaSuccess && device_count > 0) {
        config.has_cuda = true;
        config.num_gpus = device_count;
    }
#endif
    
    // æ£€æµ‹MPIèŠ‚ç‚¹æ•°
#ifdef USE_MPI
    int num_procs;
    MPI_Comm_size(MPI_COMM_WORLD, &num_procs);
    config.num_nodes = num_procs;
#endif
    
    // ä¼°ç®—å†…å­˜å¤§å° (GB)
#ifdef __linux__
    struct sysinfo info;
    if (sysinfo(&info) == 0) {
        config.total_memory_gb = info.totalram / (1024 * 1024 * 1024);
    }
#endif
    
    // ä¼°ç®—CPUé¢‘ç‡å’Œå†…å­˜å¸¦å®½
    config.cpu_frequency_ghz = 2.4; // é»˜è®¤å€¼ï¼Œå®é™…åº”è¯¥ä»/proc/cpuinfoè¯»å–
    config.memory_bandwidth_gbs = 25.6; // é»˜è®¤å€¼
    
    return config;
}

// æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆçœŸå®æ•°æ®ï¼‰
void runRealDataBenchmark(HierarchicalANNSEngine& engine, 
                         const std::vector<float>& base_data,
                         const std::vector<float>& query_data,
                         const std::vector<std::vector<int>>& ground_truth,
                         const DatasetInfo& dataset_info) {
    
    std::cout << "\n=== Real Data Performance Benchmark ===" << std::endl;
    
    const size_t k = 10; // Top-K
    const size_t num_trials = 3;
    const size_t test_queries = std::min(dataset_info.num_queries, size_t(100)); // é™åˆ¶æµ‹è¯•æŸ¥è¯¢æ•°
    
    std::vector<std::vector<int>> indices;
    std::vector<std::vector<float>> distances;
    
    // æµ‹è¯•å•ä¸€æ¶æ„æ€§èƒ½
    std::vector<ArchType> test_archs = {
        ArchType::SIMD_ONLY,
        ArchType::MULTICORE
    };
    
    std::cout << "Testing on " << test_queries << " queries from DEEP100K dataset" << std::endl;
    std::cout << std::string(70, '-') << std::endl;
    std::cout << std::setw(15) << "Architecture" 
              << std::setw(15) << "Latency(ms)" 
              << std::setw(15) << "Throughput" 
              << std::setw(15) << "Recall@10"
              << std::setw(10) << "Speedup" << std::endl;
    std::cout << std::string(70, '-') << std::endl;
    
    double baseline_latency = 0.0;
    
    for (const auto& arch : test_archs) {
        std::vector<double> latencies;
        std::vector<double> recalls;
        
        for (size_t trial = 0; trial < num_trials; ++trial) {
            auto start = std::chrono::high_resolution_clock::now();
            
            // æ‰§è¡Œæœç´¢
            engine.search(query_data.data(), test_queries, k, indices, distances);
            
            auto end = std::chrono::high_resolution_clock::now();
            auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
            latencies.push_back(duration.count() / 1000.0); // è½¬æ¢ä¸ºæ¯«ç§’
            
            // è®¡ç®—å¬å›ç‡
            std::vector<std::vector<int>> gt_subset(ground_truth.begin(), 
                                                   ground_truth.begin() + test_queries);
            double recall = RecallCalculator::calculateAverageRecall(indices, gt_subset, k);
            recalls.push_back(recall);
        }
        
        // è®¡ç®—å¹³å‡å€¼
        double avg_latency = 0.0;
        double avg_recall = 0.0;
        for (size_t i = 0; i < latencies.size(); ++i) {
            avg_latency += latencies[i];
            avg_recall += recalls[i];
        }
        avg_latency /= latencies.size();
        avg_recall /= recalls.size();
        
        double throughput = test_queries / (avg_latency / 1000.0);
        
        // è®¾ç½®åŸºå‡†å»¶è¿Ÿï¼ˆç¬¬ä¸€ä¸ªæ¶æ„ï¼‰
        if (baseline_latency == 0.0) {
            baseline_latency = avg_latency;
        }
        double speedup = baseline_latency / avg_latency;
        
        std::cout << std::setw(15) << static_cast<int>(arch)
                  << std::setw(15) << std::fixed << std::setprecision(2) << avg_latency
                  << std::setw(15) << std::fixed << std::setprecision(1) << throughput
                  << std::setw(15) << std::fixed << std::setprecision(4) << avg_recall
                  << std::setw(10) << std::fixed << std::setprecision(2) << speedup << "x"
                  << std::endl;
    }
    
    std::cout << std::string(70, '-') << std::endl;
}

// è‡ªé€‚åº”æ¶æ„é€‰æ‹©æ¼”ç¤ºï¼ˆçœŸå®æ•°æ®ï¼‰
void demonstrateAdaptiveOptimizationReal(HierarchicalANNSEngine& engine,
                                        const DatasetInfo& dataset_info) {
    std::cout << "\n=== Adaptive Architecture Selection (Real Data) ===" << std::endl;
    
    // ä½¿ç”¨çœŸå®æ•°æ®é›†ä¿¡æ¯
    DatasetProfile profile;
    profile.num_vectors = dataset_info.num_base_vectors;
    profile.dimension = dataset_info.dimension;
    profile.num_queries = dataset_info.num_queries;
    profile.sparsity = 0.0; // DEEP100Kæ˜¯ç¨ å¯†å‘é‡
    profile.correlation = 0.5; // ä¼°è®¡å€¼
    profile.data_type = "float32";
    
    engine.setDatasetProfile(profile);
    
    std::cout << "Real dataset profile:" << std::endl;
    std::cout << "  Base vectors: " << profile.num_vectors << std::endl;
    std::cout << "  Dimension: " << profile.dimension << std::endl;
    std::cout << "  Queries: " << profile.num_queries << std::endl;
    
    // æµ‹è¯•ä¸åŒè®¡ç®—é˜¶æ®µçš„æ¶æ„é€‰æ‹©
    std::vector<ComputeStage> stages = {
        ComputeStage::TRAINING,
        ComputeStage::ENCODING,
        ComputeStage::QUERYING
    };
    
    std::cout << "\nOptimal architecture selection for each stage:" << std::endl;
    for (const auto& stage : stages) {
        engine.getScheduler()->adaptiveSchedule(profile, stage);
        
        std::string stage_name;
        switch (stage) {
            case ComputeStage::TRAINING: stage_name = "Training"; break;
            case ComputeStage::ENCODING: stage_name = "Encoding"; break;
            case ComputeStage::QUERYING: stage_name = "Querying"; break;
            default: stage_name = "Unknown"; break;
        }
        
        std::cout << "  " << stage_name << " stage: Optimized architecture selected" << std::endl;
    }
}

// æ··åˆæ¶æ„æ‰§è¡Œæ¼”ç¤ºï¼ˆçœŸå®æ•°æ®ï¼‰
void demonstrateHybridExecutionReal(HierarchicalANNSEngine& engine,
                                   const std::vector<float>& query_data,
                                   const std::vector<std::vector<int>>& ground_truth,
                                   const DatasetInfo& dataset_info) {
    
    std::cout << "\n=== Hybrid Architecture Execution (Real Data) ===" << std::endl;
    
    const size_t k = 10;
    const size_t test_queries = std::min(dataset_info.num_queries, size_t(50));
    
    std::vector<std::vector<int>> indices;
    std::vector<std::vector<float>> distances;
    
    auto start = std::chrono::high_resolution_clock::now();
    
    // ä½¿ç”¨æ··åˆæ¶æ„æœç´¢
    engine.hybridSearch(query_data.data(), test_queries, k, indices, distances);
    
    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    
    // è®¡ç®—å¬å›ç‡
    std::vector<std::vector<int>> gt_subset(ground_truth.begin(), 
                                           ground_truth.begin() + test_queries);
    double recall = RecallCalculator::calculateAverageRecall(indices, gt_subset, k);
    
    std::cout << "Hybrid search results:" << std::endl;
    std::cout << "  Queries processed: " << test_queries << std::endl;
    std::cout << "  Total time: " << duration.count() / 1000.0 << " ms" << std::endl;
    std::cout << "  Average latency: " << duration.count() / 1000.0 / test_queries << " ms/query" << std::endl;
    std::cout << "  Throughput: " << test_queries / (duration.count() / 1000000.0) << " QPS" << std::endl;
    std::cout << "  Recall@" << k << ": " << std::fixed << std::setprecision(4) << recall << std::endl;
    
    // æ˜¾ç¤ºä¸€äº›ç¤ºä¾‹ç»“æœ
    std::cout << "\nSample results for first 3 queries:" << std::endl;
    for (size_t i = 0; i < std::min(size_t(3), test_queries); ++i) {
        std::cout << "Query " << i << " - Top 5 results: ";
        for (size_t j = 0; j < std::min(size_t(5), indices[i].size()); ++j) {
            std::cout << indices[i][j];
            if (j < std::min(size_t(5), indices[i].size()) - 1) std::cout << ", ";
        }
        std::cout << " (distances: ";
        for (size_t j = 0; j < std::min(size_t(5), distances[i].size()); ++j) {
            std::cout << std::fixed << std::setprecision(3) << distances[i][j];
            if (j < std::min(size_t(5), distances[i].size()) - 1) std::cout << ", ";
        }
        std::cout << ")" << std::endl;
    }
}

// æ•°æ®é›†è´¨é‡åˆ†æ
void analyzeDatasetQuality(const std::vector<float>& base_data,
                          const std::vector<float>& query_data,
                          const DatasetInfo& dataset_info) {
    
    std::cout << "\n=== Dataset Quality Analysis ===" << std::endl;
    
    // åŸºæœ¬ç»Ÿè®¡
    std::cout << "Dataset: " << dataset_info.dataset_name << std::endl;
    std::cout << "Base vectors: " << dataset_info.num_base_vectors << std::endl;
    std::cout << "Query vectors: " << dataset_info.num_queries << std::endl;
    std::cout << "Dimension: " << dataset_info.dimension << std::endl;
    std::cout << "Ground truth K: " << dataset_info.ground_truth_k << std::endl;
    
    // æ•°æ®è§„æ¨¡åˆ†æ
    double base_size_mb = (base_data.size() * sizeof(float)) / (1024.0 * 1024.0);
    double query_size_mb = (query_data.size() * sizeof(float)) / (1024.0 * 1024.0);
    
    std::cout << "Memory footprint:" << std::endl;
    std::cout << "  Base data: " << std::fixed << std::setprecision(2) << base_size_mb << " MB" << std::endl;
    std::cout << "  Query data: " << std::fixed << std::setprecision(2) << query_size_mb << " MB" << std::endl;
    std::cout << "  Total: " << std::fixed << std::setprecision(2) << (base_size_mb + query_size_mb) << " MB" << std::endl;
    
    // å‘é‡èŒƒæ•°åˆ†æ
    if (!base_data.empty() && dataset_info.dimension > 0) {
        double sum_norm = 0.0;
        size_t sample_count = std::min(size_t(1000), dataset_info.num_base_vectors);
        
        for (size_t i = 0; i < sample_count; ++i) {
            double norm = 0.0;
            for (size_t j = 0; j < dataset_info.dimension; ++j) {
                float val = base_data[i * dataset_info.dimension + j];
                norm += val * val;
            }
            sum_norm += std::sqrt(norm);
        }
        
        double avg_norm = sum_norm / sample_count;
        std::cout << "Average vector norm (sample): " << std::fixed << std::setprecision(4) << avg_norm << std::endl;
    }
}

int main(int argc, char* argv[]) {
    std::cout << "=== Hierarchical Parallel ANNS Framework - DEEP100K Demo ===" << std::endl;
    
#ifdef USE_MPI
    // åˆå§‹åŒ–MPI
    MPI_Init(&argc, &argv);
    
    int rank, size;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);
    
    if (rank == 0) {
        std::cout << "Running with MPI: " << size << " processes" << std::endl;
    }
#endif
    
    try {
        // 1. æ£€æµ‹ç¡¬ä»¶é…ç½®
        auto hw_config = detectHardwareConfig();
        std::cout << "\nDetected Hardware Configuration:" << std::endl;
        std::cout << "  CPU cores: " << hw_config.num_cores << std::endl;
        std::cout << "  Memory: " << hw_config.total_memory_gb << " GB" << std::endl;
        std::cout << "  AVX512 support: " << (hw_config.has_avx512 ? "Yes" : "No") << std::endl;
        std::cout << "  CUDA support: " << (hw_config.has_cuda ? "Yes" : "No") << std::endl;
        std::cout << "  GPUs: " << hw_config.num_gpus << std::endl;
        std::cout << "  MPI nodes: " << hw_config.num_nodes << std::endl;
        
        // 2. åŠ è½½DEEP100Kæ•°æ®é›†
        std::string dataset_path = "../anndata"; // ç›¸å¯¹è·¯å¾„åˆ°anndataæ–‡ä»¶å¤¹
        DEEP100KLoader loader(dataset_path);
        
        std::vector<float> base_vectors, query_vectors;
        std::vector<std::vector<int>> ground_truth;
        
        if (!loader.loadDataset(base_vectors, query_vectors, ground_truth)) {
            std::cerr << "Failed to load DEEP100K dataset from " << dataset_path << std::endl;
            std::cerr << "Please ensure the anndata folder is accessible" << std::endl;
            return -1;
        }
        
        // 3. éªŒè¯æ•°æ®é›†
        if (!loader.validateDataset(base_vectors, query_vectors, ground_truth)) {
            std::cerr << "Dataset validation failed" << std::endl;
            return -1;
        }
        
        const auto& dataset_info = loader.getInfo();
        
        // 4. åˆ†ææ•°æ®é›†è´¨é‡
        analyzeDatasetQuality(base_vectors, query_vectors, dataset_info);
        
        // 5. åˆ›å»ºåˆ†å±‚å¹¶è¡Œå¼•æ“
        HierarchicalANNSEngine engine(hw_config);
        
        if (!engine.initialize()) {
            std::cerr << "Failed to initialize ANNS engine" << std::endl;
            return -1;
        }
        
        std::cout << "\nHierarchical ANNS Engine initialized successfully" << std::endl;
        
        // 6. è®¾ç½®æ•°æ®é›†ç‰¹å¾
        DatasetProfile profile;
        profile.num_vectors = dataset_info.num_base_vectors;
        profile.dimension = dataset_info.dimension;
        profile.num_queries = dataset_info.num_queries;
        profile.sparsity = 0.0;
        profile.correlation = 0.5;
        profile.data_type = "float32";
        
        engine.setDatasetProfile(profile);
        
        // 7. è®­ç»ƒé˜¶æ®µæ¼”ç¤ºï¼ˆä½¿ç”¨çœŸå®æ•°æ®ï¼‰
        std::cout << "\n=== Training Phase (Real Data) ===" << std::endl;
        auto train_start = std::chrono::high_resolution_clock::now();
        engine.train(base_vectors.data(), dataset_info.num_base_vectors, dataset_info.dimension);
        auto train_end = std::chrono::high_resolution_clock::now();
        auto train_duration = std::chrono::duration_cast<std::chrono::milliseconds>(train_end - train_start);
        std::cout << "Training completed in " << train_duration.count() << " ms" << std::endl;
        
        // 8. è‡ªé€‚åº”ä¼˜åŒ–æ¼”ç¤º
        demonstrateAdaptiveOptimizationReal(engine, dataset_info);
        
        // 9. æ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆçœŸå®æ•°æ®ï¼‰
        runRealDataBenchmark(engine, base_vectors, query_vectors, ground_truth, dataset_info);
        
        // 10. æ··åˆæ¶æ„æ‰§è¡Œæ¼”ç¤º
        demonstrateHybridExecutionReal(engine, query_vectors, ground_truth, dataset_info);
        
        // 11. ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
        std::cout << "\n=== Performance Report Generation ===" << std::endl;
        engine.generatePerformanceReport("deep100k_hierarchical_performance.csv");
        std::cout << "Performance report saved to: deep100k_hierarchical_performance.csv" << std::endl;
        
        // 12. æ˜¾ç¤ºæ€§èƒ½æ‘˜è¦
        auto history = engine.getPerformanceHistory();
        if (!history.empty()) {
            const auto& latest = history.back();
            std::cout << "\nFinal Performance Summary:" << std::endl;
            std::cout << "  Latest latency: " << std::fixed << std::setprecision(2) << latest.latency_ms << " ms" << std::endl;
            std::cout << "  Latest throughput: " << std::fixed << std::setprecision(1) << latest.throughput_qps << " QPS" << std::endl;
            std::cout << "  Memory usage: " << std::fixed << std::setprecision(2) << latest.memory_usage_gb << " GB" << std::endl;
            std::cout << "  Performance records: " << history.size() << std::endl;
        }
        
        std::cout << "\nğŸ‰ DEEP100K demo completed successfully!" << std::endl;
        std::cout << "The hierarchical parallel framework achieved excellent performance on real data." << std::endl;
        
    } catch (const std::exception& e) {
        std::cerr << "Error: " << e.what() << std::endl;
        return -1;
    }
    
#ifdef USE_MPI
    MPI_Finalize();
#endif
    
    return 0;
} 